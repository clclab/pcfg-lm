PLANS

[This file is currently living in /u/nlp/java/edu/stanford/nlp/ie/PLANS
which isn't necessarily the correct place, but it seemed an okay one.]


OBJECTIVES

Publishable research!

Good general information extraction [KDD, PADLR, Edinburgh]

Be able to effectively extract relational data from webpages [KDD, PADLR]

Integration with Semantic Web, RDF, OWL [ex-DAML+OIL], KAON [KDD, PADLR]

State of the art Machine learning of named-entity recognizers (character
level stuff, or also explore word level further?) [Edinbugh, KDD]

Bootstrapping/cotraining from minimal annotated data [KDD, Edinburgh]

Integration with Edinburgh XML toolset [Edinburgh]

Focus on particular kinds of relations for education: prerequisites,
type of resource, quality, level.  Some text classification tasks [PADLR]


SHORT TERM WORD-LEVEL HMM CODE PLANS

Corpus.java: Revise: Make corpus reading 10 times faster (or at any rate,
I think it's very inefficient now).  If possible, integrate it with the
emerging edu.stanford.nlp.dbm.DataCollection/FileDataCollection .
Part of this is being able to handle different document formats. While
it would be useful to continue to be able to handle the quirky format of
acquisitions.txt, it'd also be useful to be able to handle other formats
(e.g., real XML).

StructureLearning: We need to have some kind of Bayesian/MDL prior for
structure learning implemented (or else to do validation against held
out data, which should have the same effect of controlling growth).
Indeed one should have it do crossvalidation and several runs on
different structures (as Fr&McC did) to lessen the effects of bad EM
maxima or split variation.

FieldExtractor.java: Sort out the relation between
edu.stanford.nlp.ie.FieldExtractor.java and
edu.stanford.nlp.ie.hmm.FieldExtractor.java, and probably delete the latter.

Structure: Make it so (Target)Structure can be a general graph and
general graph structures are explored.  (At least optionally: this
will expand the state space.)  Perhaps delete TargetStructure and just
have it as part of the functionality of Structure -- in principle it's
just a subset of the functionality, and one would just have particular
structure building methods for that case.  On the other hand, a target
strucure is small enough that one can clearly do exhaustive search for
a good model structure, whereas one couldn't for a large graph.

HMM: Maybe change it so that there are start and finish observations
emitted by a ConstantEmitMap.  I think this would remove a lot of the
special case stuff from the core HMM code.

Fix up unseen word probs so they are sensible in all circumstances.  At
present the unseen probability effectively means one gets linear
discounting of seen probabilities, which is _very_ bad.  But one also
needs a sensible model of unseen-for-state and completely-unseen word
probabilities, and I'm actually not quite sure what it is.  Doing this
right seems quite important to performance.  Set unseen word type
(Feature(Map) probabilities also from observations in the validation
data.  [The treatment of unseen words is subtle: you want it to
recognize that capitalized words appear in certain target states, but
you don't want their estimates for capitalized words to get so much
higher than the background state that it is systematically overproposing
fillers just because their target emission probability is so high.  Use
a more principled method to delete words from emissions tables whose
reestimated probability is very small.

Make it possible so that after using the held-out data to train
various parameters that one retrains using all of the data for a final
model.  This should help a fair bit in avoiding data sparseness.

Round out and clean up Extractor/Trainer stuff (all the main methods) so
one can flexibly train and test and save HMMs with different structures
-- e.g. a Context HMM which has some prefix and suffix states.  Probably
Extractor should actually use Trainer and Tester, as then there might be
less bad overlap of code.  ContextStructureLearner needs to exist as
functionality (if not as a separate class).

Explore using the target/context training with simultaneous models of
different classes to get better results.  Learning a model of all
states at once like this, seems key, and quite possible.

Get much better results than Fr&McC during this quarter.

At the moment one goes through test document and can find multiple
instances of target, and take highest probability one (on a fairly local
basis).  You could change things (by expanding the state space) so that
one could only identify one target in the document.  Would that work
better?  It might.  Think about it.  (One could also expand the current
window to do from background state to background state, but presumably
this becomes increasingly dominated by the probabilities of the various
words.  Do Freitag & McCallum say clearly what they did?) 

Should probably get rid of calcEmitStates notion from HMM.java, and just
use the principle that one doesn't reestimate ConstantEmitMap's.

Hook the HMM stuff up to some FSM drawing tool (such as AT&T's dotty? or
VCG) so one can easily see HMMs.


SHORT TERM CHARACTER-LEVEL HMM CODE PLANS

Reimplement it as a model that has segmentation and classification.  (Do
segmentation first as a separate step?)

Get clear where one can argue advantages over the Cucerzan and Yarowsky
work (presuming one can).

Learn character classes?

Integrate in with word-level code as an alternative unseen estimator.

Run on more standard data sets

Put under edu.stanford.nlp.ie as a separate package.  Make it callable
-- see what Tim Grow did.  Check his claims that the results weren't
always very sensible.



LONGER TERM PLANS

Bootstrapping/co-training

Syntactic parsing information extraction

Investigating other techniques

Make a graphical colour coded display of IE output (built on top of
either Miler's annotation tool or Ontomat?).  See Poibeau paper.


LITERATURE

http://www.cs.jhu.edu/~yarowsky/pubs.html

Cucerzan, S. and D. Yarowsky. ``Language Independent Named Entity
Recognition Combining Morphological and Contextual Evidence.'' In
Proceedings, 1999 Joint SIGDAT Conference on Empirical Methods in NLP
and Very Large Corpora, pp. 90-99, 1999. 

Cucerzan, S. and D. Yarowsky, `` Language independent minimally
supervised induction of lexical probabilities.'' Proceedings of
ACL-2000, Hong Kong, pages 270-277, 2000. 


http://www-2.cs.cmu.edu/afs/cs/user/dayne/www/cv.html

D. Freitag and A. McCallum, "Information extraction with HMM structures
learned by stochastic optimization," Proceedings of AAAI-2000.

D. Freitag and A. McCallum, "Information extraction using HMMs and
shrinkage," Proceedings of the AAAI-99 Workshop on Machine Learning for
Information Extraction.

D. Freitag, "Machine Learning for Information Extraction in Informal
Domains," PhD. dissertation, November, 1998.


http://www-2.cs.cmu.edu/~kseymore/papers.html

Learning Hidden Markov Model Structure for Information Extraction
Kristie Seymore, Andrew McCallum, and Ronald Rosenfeld.
AAAI'99 Workshop on Machine Learning for Information Extraction. 1999.


S. Ray & M. Craven (2001). Representing Sentence Structure in Hidden
Markov Models for Information Extraction. Proceedings of the 17th
International Joint Conference on Artificial Intelligence, Seattle,
WA. Morgan Kaufmann.


CoNLL shared task

http://lcg-www.uia.ac.be/conll2002/ner/
[At some point it would be useful to benchmark how well we perform on
this data, but maybe it is better to wait so we're not overtraining on
every data set.]


BBN papers (HMM named entity recognizer and parsing for IE)

D. Bikel, R. Schwartz, R. Weischedel, "An Algorithm that Learns What's
in a Name," Machine Learning,
1999. http://citeseer.nj.nec.com/bikel99algorithm.html    

@inproceedings{ miller98algorithms,
  author = "Scott Miller and Michael Crystal and Heidi Fox and Lance
Ramshaw and Richard Schwartz and Rebecca Stone and Ralph Weischedel and
the Annotation Group",
  title = "Algorithms that learn to extract information--{BBN}:
Description of the {SIFT} system as used for {MUC}",
  booktitle="{Proceedings of the Seventh Message Understanding
Conference (MUC-7)}",
  year = "1998",
  url = "citeseer.nj.nec.com/miller98algorithms.html" }

Michael Collins and Scott Miller. 1998. Semantic Tagging using a
Probabilistic Context Free Grammar. In Proceedings of the Sixth Workshop
on Very Large Corpora. 

Scott Miller, Heidi Fox, Lance Ramshaw and Ralph Weischedel, A Novel Use
of Statistical Parsing to Extract Information from Text, Proceedings of
the 1st Annual Meeting of the North American Chapter of the ACL (NAACL),
p. 226 - 233, Seattle, Washington, 2000  

HMMs

Rabiner Tutorial:
http://cmgm.stanford.edu/biochem218/rabiner.pdf
A Tutorial On Hidden Markov Models and Selected Applications in Speech
Recognition. Lawrence R Rabiner.

"Corrections" page for Rabiner scaling mentioned in the code has been
"withdrawn":

	http://web.media.mit.edu/~rahimi/rabiner/

but is still available here:

     http://www.media.mit.edu/~rahimi/rabiner/rabiner-errata/

and at any rate, the reference below also sees themselves as correcting
the scaling in the Rabiner paper -- at any rate, one wants to have the
math correct in one way or another (I believe it currently is correct at
the basic forwards-backwards level).

Very detailed German presentation of HMM math, including scaling
factors:
http://santana.uni-muenster.de/Publications/ehmmmath.ps

